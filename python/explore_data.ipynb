{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ndjson\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data using ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    data_fnames = os.listdir(data_dir)\n",
    "    A = [0 for i in range(len(data_fnames))]\n",
    "    \n",
    "    for i, fname in enumerate(data_fnames):\n",
    "\n",
    "        # Get filepath\n",
    "        full_path = os.path.join(data_dir, fname)\n",
    "\n",
    "        # Load file\n",
    "        with open(full_path, \"rb\") as f:\n",
    "            A[i] = ndjson.load(f)\n",
    "            f.close()\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Strokes\n",
    "We'll use matplotlib to visualize series of strokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_strokes(drawing):\n",
    "    sqrt_N = math.ceil(np.sqrt(len(drawing)))\n",
    "    fig, axs = plt.subplots(sqrt_N, sqrt_N, sharex=True, sharey=True)\n",
    "    xplot_index = 0\n",
    "    yplot_index = 0\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for j, stroke in enumerate(drawing):\n",
    "        xs.append(stroke[0])\n",
    "        ys.append(stroke[1])\n",
    "        xs = list(itertools.chain.from_iterable([xs[:-1],xs[-1]]))\n",
    "        ys = list(itertools.chain.from_iterable([ys[:-1],ys[-1]]))\n",
    "        if j % sqrt_N == 0 and j != 0:\n",
    "            xplot_index = 0\n",
    "            yplot_index += 1\n",
    "        axs[yplot_index, xplot_index].plot(xs, ys)\n",
    "        axs[yplot_index, xplot_index].set_title(\"Drawing {}\".format((xplot_index+1) + sqrt_N*yplot_index))\n",
    "\n",
    "        xplot_index += 1\n",
    "    plt.subplots_adjust(top=1.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Drawings and Labels\n",
    "This is where we'll start to build our supervised learning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drawings_and_labels(data):\n",
    "    drawings = []\n",
    "    classes = []\n",
    "    class2index = {}\n",
    "    for class_index in range(len(A)):\n",
    "        class2index[class_index] = A[class_index][0]['word']\n",
    "        for drawing_index in range(len(A[class_index])):\n",
    "            drawings.append(A[class_index][drawing_index]['drawing'])\n",
    "            classes.append(class_index)\n",
    "    return drawings, classes, class2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class indices to classes mapping: {0: 'cat', 1: 'table'}\n"
     ]
    }
   ],
   "source": [
    "data = load_data(data_dir)\n",
    "drawings, classes, class2index = get_drawings_and_labels(data)\n",
    "print(\"Class indices to classes mapping: {}\".format(class2index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a PyTorch DataLoader Using Drawings\n",
    "Let's transform our dataset into a PyTorch DataLoader so we can begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Now let's create a DataLoader class\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.N)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return data[index], label[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Training\n",
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.8\n",
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training, Testing, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index of shuffled data to split\n",
    "N = len(drawings)\n",
    "indices = [i for i in range(N)]\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Randomly shuffle drawings and labels\n",
    "drawings = np.array(drawings)[indices]\n",
    "classes = np.array(classes)[indices]\n",
    "\n",
    "split = int(N * TRAIN_TEST_SPLIT)\n",
    "\n",
    "# Now split data\n",
    "train_data = drawings[:split]\n",
    "train_labels = classes[:split]\n",
    "\n",
    "val_data = drawings[split:split + N // 10]\n",
    "val_labels = classes[split:split + N // 10]\n",
    "\n",
    "\n",
    "test_data = drawings[split + N // 10:]\n",
    "test_labels = classes[split + N // 10:]\n",
    "\n",
    "# Now create Datasets - TODO: Format input arguments correctly\n",
    "train_dataset = QuickDrawDataset(train_data, train_labels)\n",
    "val_dataset = QuickDrawDataset(val_data, val_labels)\n",
    "test_dataset = QuickDrawDataset(test_data, test_labels)\n",
    "\n",
    "# Now create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251223,)\n"
     ]
    }
   ],
   "source": [
    "print(.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pragmatics",
   "language": "python",
   "name": "pragmatics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
